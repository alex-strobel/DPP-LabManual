# Meta-Analysis

[Alexander Strobel](mailto:alexander.strobel@tu-dresden.de)

# Introduction

As with every research project, one tenet of conductiong a meta-analysis is that our procedure of doing so is described as transparently and replicably as possible. I am (by 2022-01-12) rather unexperienced in meta-analysis, but from my reading, the major issues seem to be: 

+ [clearly defined search terms for all eligible databases of interest (e.g., PsycInfo, Web of Knowledge, PubMed, Google Scholar etc.)](#search-terms)
+ further inclusion and exclusion criteria including
+ rules for documenting inclusion and exclusion
+ manualized coding schemes for effect sizes and study characteristics
+ choice of the appropriate software tool and analysis specifications for conduction a meta-analysis
+ assessment of potential moderators (i.e., meta-regression)
+ asessment of potential publication bias


# Search terms

Yet, (for me and so far) it seems that for meta-analyses, the transparent and replicable way of conducting them (e.g., via a predefined and more or less sophisticated Boolean search term on *PsycInfo*) leaves one with far fewer eligible studies for inclusion than some idiosyncratic way of browsing through hundreds of papers on *Google Scholar*, going through their abstracts, hopefully also their fulltexts, checking their references, then being reminded of some other study of a colleague where the research focus was different, but the variables in question were assessed, and then being reminded of own studies to which the same applies ...

As an example, I was about to perform a meta-analysis on the relationship of *Need for Cognition* and *Neuroticism*, and I thought this a rather cheap one when it comes to meta-analysis. I spent two or three days on the results of a *Google Scholar* search for the search terms `"Need for Cognition" Neuroticism` which gave me about 4.000 hits. I scanned through the titles, abstracts, and – where possible – full-texts, and after about 110 database entries, I had identified 35 studies and thought that this could'nt be the correct (i.e., replicable) way of doing so. 

I therefore consulted a recent meta-analysis of someone I trusted to do a proper literature search (<!-- add  ref to Buecker et al. -->). Using *PsycInfo*, I entered an - even more elaborate - Boolean search term `(TI "need for cognition" OR AB "need for cognition") AND (AB (neuroticism OR "emotional stability"))`. This resulted in exactly 20 hits (per 2022-01-12). There was some overlap with my *Google Scholar* hit list, but many relavant papers were not found, while a couple of irrelevant papers emerged.

So what to do in this case? One could adjust the search terms, add some restrictions here and some OR operator there.... which is what I did, but either this resulted in even fewer or an astonishingly larger number of records. Every search term combination I applied did not result in having all the 35+ relevant papers shown up in the results in my search.

Well, I trusted my search, as it gave me 35 papers of relevance for my research question (plus about two dozens of papers where the relevant variables were assessed, but their bivariate correlations were not reported, so I set them on a list for data requests). But still, my way of searching (and finding) results was by no means transparent or reproducible. Also, I gave up to search any longer after 110 out of 4000 results. *Google Scholar* per default lists its results by relevance, and between the 90th an 110th result, I encountered a considerable drop in - to my endeavour - relevant results.